# -*- coding: utf-8 -*-
"""Timelines_NYT_Web_Scraper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/154ZHPcuQ2PbefdSRJD3d22mSpidhFAIX

**Packages & Libraries**
"""

# install the necessary packages
! pip install pynytimes
! pip install nltk
! pip install newspaper3k

# import libraries
import requests as req
import time
from pynytimes import NYTAPI
import datetime
import nltk 
import newspaper
from textblob import TextBlob
from newspaper import Article
from nltk.corpus import stopwords
import spacy
from pprint import pprint
nltk.download('punkt')
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""**Scraping Articles**"""

# Web Scrape Searcher
nyt = NYTAPI(API_KEY, parse_dates=True)

# scraping the articles
articles1 = nyt.article_search(
    query = "Bush Saddam Hussein",
    results = 2000,
    dates = {
        "begin": datetime.datetime(2003, 3, 20),
        "end": datetime.datetime(2004, 3, 20)
    },
    options = {
        "sort": "oldest"
    }
)

articles1

df1 = pd.DataFrame(articles1)
df1

# applying NLP and the newspaper package to the links in the dataframe
stories1 = []
links = list(df1['web_url'])
for link in links:
  try: 
    article = Article(link)

    article.download()
    article.parse()
    article.nlp()

    url = article.url
    title = article.title
    authors = article.authors
    keywords = article.keywords
    publish_date = article.publish_date
    summary = article.summary
    text = article.text

    print('\n')
    print(f'URL: {url}')
    print(f'Title: {title}')
    print(f'Authors: {authors}')
    print(f'Keywords: {keywords}')
    print(f'Publish Date: {publish_date}')
    print(f'Summary: {summary}')
    print(f'Text: {text}')

    report = {'Title':title, 'URL':url, 'Authors':authors, 'Keywords':keywords, 'Publish Date':publish_date, 'Summary':summary, 'Text':text}
    stories1.append(report)

  except:
    continue

timeline1 = pd.DataFrame(stories1)
timeline1

"""**Text Mining**"""

timeline1

## Data Transformation
# lower casing the text

timeline1['Text'] = timeline1['Text'].astype(str).str.lower()
timeline1

## Tokenization

from nltk.tokenize import RegexpTokenizer
regexp = RegexpTokenizer('\w+')

timeline1['Text_Token'] = timeline1['Text'].apply(regexp.tokenize)
timeline1

## Stopwords

nltk.download('stopwords')
from nltk.corpus import stopwords

# make a list of English stopwords
stopwords = nltk.corpus.stopwords.words("english")

# extend the list with your own custom stopwords
# eliminating the publication name
my_stopwords = ['new york times', 'mr', 'said', 'one', 'president']
stopwords.extend(my_stopwords)

# removing the stopwords
timeline1['Text_Token'] = timeline1['Text_Token'].apply(lambda x: [item for item in x if item not in stopwords])
timeline1

## Removing Infrequent Words

# change the format of Text_Token to strings and keep only words longer than 2 letters
timeline1['Text_String'] = timeline1['Text_Token'].apply(lambda x: ' '.join([item for item in x if len(item)>2]))

# create a list of all words
all_words = ' '.join([word for word in timeline1['Text_String']])

# tokenize all_words
tokenized_words = nltk.tokenize.word_tokenize(all_words)

# Frequency Distribution
from nltk.probability import FreqDist
fdist = FreqDist(tokenized_words)
fdist

# drop words which occur less than a certain amount of times
timeline1['Text_String_Fdist'] = timeline1['Text_Token'].apply(lambda x: ' '.join([item for item in x if fdist[item] >= 3]))
timeline1

## Lemmatization

nltk.download('wordnet')
nltk.download('omw-1.4')

from nltk.stem import WordNetLemmatizer

wordnet_lem = WordNetLemmatizer()

timeline1['Text_String_Lem'] = timeline1['Text_String_Fdist'].apply(wordnet_lem.lemmatize)

timeline1['is_equal'] = (timeline1['Text_String_Fdist'] == timeline1['Text_String_Lem'])
timeline1.is_equal.value_counts()

"""**Keyword Frequency Analysis**"""

## Word Cloud

all_words_lem = ' '.join([word for word in timeline1['Text_String_Lem']])

from wordcloud import WordCloud

wordcloud = WordCloud(width=600, 
                     height=400, 
                     random_state=2, 
                     max_font_size=100).generate(all_words_lem)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')

timeline1

## Frequency Distribution

from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist

words = nltk.word_tokenize(all_words_lem)
fd = FreqDist(words)

fd.most_common(3)

## Plot Common Words

# Obtain top 10 words
top_10 = fd.most_common(10)

# create a apandas series to make plotting easier
fdist = pd.Series(dict(top_10))

# visualize
sns.barplot(y=fdist.index, x=fdist.values)

"""**Sentiment Analysis**"""

# download VADER
nltk.download('vader_lexicon')

# Intensity Analyzer
from nltk.sentiment import SentimentIntensityAnalyzer

analyzer = SentimentIntensityAnalyzer()

# Polarity Scores
timeline1['Polarity'] = timeline1['Text_String_Lem'].apply(lambda x: analyzer.polarity_scores(x))
timeline1

# Transform / Change data structure
timeline1 = pd.concat(
    [timeline1.drop(['Polarity'], axis=1), 
     timeline1['Polarity'].apply(pd.Series)], axis=1)
timeline1

# Create a new variable with sentiment "neutral," "positive," and "negative"
timeline1['Sentiment'] = timeline1['compound'].apply(lambda x: 'positive' if x>0 else 'neutral' if x==0 else 'negative')
timeline1

## Exploratory Analysis

# Visualizing the number of articles by sentiment
sns.countplot(y='Sentiment', data=timeline1)

## Exploratory Analysis

# creating a Boxplot to show distribution of sentiment
sns.boxplot(y='compound', x='Sentiment', data=timeline1)

"""**Topic Modeling**"""

! pip install pyLDAvis

# Import Libraries

from pprint import pprint# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel# spaCy for preprocessing
import spacy# Plotting tools
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
nlp = spacy.load('en_core_web_sm')

timeline1

# Build the bigram and trigram models
bigram = gensim.models.Phrases(timeline1['Text_Token'], min_count=1, threshold=2) # higher threshold fewer phrases.
trigram = gensim.models.Phrases(bigram[timeline1['Text_Token']], threshold=2)

# Faster way to get a sentence clubbed as a trigram/bigram
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)

# See trigram example
print(trigram_mod[bigram_mod[timeline1['Text_Token'][0]]])

## Create the Dictionary and Corpus needed

# create dictionary
id2word = corpora.Dictionary(timeline1['Text_Token'])

# create corpus
texts = timeline1['Text_Token']

# Term Document Frequency 
corpus = [id2word.doc2bow(text) for text in texts]  

# View 
print(corpus[:1]) # gives each word an id and prints the amount of times that word appears
[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]] # a more readable format of the view

## Building Topic Model

# Alpha and Beta are Hyperparameters
# Alpha --> document-topic density
# Beta --> topic-word density
# chunksize --> no. of documents to be used in each training chunk
# update_every --> how often the model parameters should be updated
# passes --> total no. of training passes

# model (10 topics):
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=10, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto',
                                           per_word_topics=True)

## Building the Topic Model

# print the keywords of topics
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]

## Building the Topic Model

# Compute Perplexity
print('\nPerplexity: ', lda_model.log_perplexity(corpus))  
# a measure of how good the model is. lower the better.

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, texts=timeline1['Text_Token'], dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)
# a measure of how human interpretable it is. higher coherence = more interpretable

## Building the Topic Model

# creating a visualization
pyLDAvis.enable_notebook()
pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)